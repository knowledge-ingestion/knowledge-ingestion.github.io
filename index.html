<!DOCTYPE html>
<html>
<head>
  <title>Synthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Synthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=LiDm8jEAAAAJ&hl=en" target="_blank">Jiaxin Zhang</a><sup>1,2</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=qsuYsLsAAAAJ&hl=en" target="_blank">Wendi Cui</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://www.linkedin.com/in/yi-ran-huang/" target="_blank">Yiran Huang</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?hl=en&user=AF6kWHUAAAAJ" target="_blank">Kamalika Das</a><sup>1,2</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?hl=en&user=KHtiAOMAAAAJ" target="_blank">Sricharan Kumar</a><sup>1,2</sup>,</span>                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Intuit AI Research,</span>
                    <span class="author-block"><sup>2</sup>Intuit</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2410.09629" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.intuit.com/AIResearch/Knowledge-injection" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2410.09629" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img *src*="static/images/carousel1.jpg" *alt*="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus.
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large language models (LLMs) are proficient in capturing factual knowledge across various domains. However, refining their capabilities on previously seen knowledge or integrating new knowledge from external sources remains a significant challenge. In this work, we propose a novel synthetic knowledge ingestion method called <i>Ski</i>, which leverages fine-grained synthesis, interleaved generation, and assemble augmentation strategies to construct high-quality data representations from raw knowledge sources. We then integrate <i>Ski</i> and its variations with three knowledge injection techniques: Retrieval Augmented Generation (RAG), Supervised Fine-tuning (SFT), and Continual Pre-training (CPT) to inject and refine knowledge in language models. Extensive empirical experiments are conducted on various question-answering tasks spanning finance, biomedicine, and open-generation domains to demonstrate that <i>Ski</i> significantly out-performs baseline methods by facilitating effective knowledge injection. We believe that our work is an important step towards enhancing the factual accuracy of LLM outputs by refining knowledge representation and injection capabilities.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img class="smaller-image" src="static/images/overview1.jpg" alt="Overview 1"/>
        <h2 class="subtitle has-text-centered">
          Base models are often unable to handle certain questions due to limited knowledge. Even with raw knowledge provided, the output answer may still be incorrect if not well-digested by LLMs. Our proposed Synthetic Knowledge Ingestion method, <i>Ski</i>, incorporates three key innovations to easily transform raw knowledge into refined data representations that LLM can effectively digest. By utilizing injection pipelines such as RAG, SFT, and CPT, knowledge or information will be injected into LLM to ensure accurate and correct answers.
        </h2>
      </div>
      <div class="item">
        <img class="smaller-image" src="static/images/overview2.jpg" alt="Overview 2"/>
        <h2 class="subtitle has-text-centered">
          Overview of the proposed method: Synthetic Knowledge Ingestion (<i>Ski</i>), comprises of three essential components. Firstly, fine-grained synthesis incorporates generating hypothetical questions based on an <i>n</i>-gram detail-oriented principal. Secondly, interleaved generation generates questions and answers simultaneously by maintaining aligned harmony, given a specific knowledge piece. Lastly, assemble augmentation combines question, answer, and context pairs, along with <i>n</i>-gram synthesis, to improve the repetition with diversity.
        </h2>
      </div>
    </div>
  </div>
</div>
</div>
</section> -->
<!-- End image carousel -->

<section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title">Introduction</h2>
    Large language models (LLMs) demonstrate proficiency in capturing vast amounts of factual information across a wide range of fields, attributable to their extensive pre-training datasets. Although these models hold an impressive repository of knowledge, integrating new information via external datasets or enhancing their capacity on previously seen information still poses several challenges. One primary challenge is outdated knowledge, where the static nature of the information fails to evolve over time. Another issue is the domain knowledge deficit where language models, typically generalists, lack detailed, specialized knowledge in sectors like finance and healthcare. Additionally, there is the problem of catastrophic forgetting, where language models may lose previously acquired knowledge, which particularly affects rare facts that are minimally represented in the training data. These issues underscore the necessity for ongoing enhancements to the knowledge capabilities.
  </div>
</section>

<!-- Image section -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="image-section">
        <div class="image-item">
          <figure class="image">
            <img class="smaller-image" src="static/images/overview1.jpg" alt="Overview 1"/>
          </figure>
          <div class="container is-max-desktop content">
            <p class="smaller-text">
              Base models are often unable to handle certain questions due to limited knowledge. Even with raw knowledge provided, the output answer may still be incorrect if not well-digested by LLMs. Our proposed Synthetic Knowledge Ingestion method, <i>Ski</i>, incorporates three key innovations to easily transform raw knowledge into refined data representations that LLM can effectively digest. By utilizing injection pipelines such as RAG, SFT, and CPT, knowledge or information will be injected into LLM to ensure accurate and correct answers.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
  <br></br>
  <div class="hero-body">
    <div class="container">
      <div class="image-section">
        <div class="image-item">
          <figure class="image">
            <img class="smaller-image" src="static/images/overview2.jpg" alt="Overview 2"/>
          </figure>
          <div class="container is-max-desktop content">
            <p class="smaller-text">
              Overview of the proposed method: Synthetic Knowledge Ingestion (<i>Ski</i>), comprises of three essential components. Firstly, fine-grained synthesis incorporates generating hypothetical questions based on an <i>n</i>-gram detail-oriented principal. Secondly, interleaved generation generates questions and answers simultaneously by maintaining aligned harmony, given a specific knowledge piece. Lastly, assemble augmentation combines question, answer, and context pairs, along with <i>n</i>-gram synthesis, to improve the repetition with diversity.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End image section -->

<!-- Paper content -->
<section class="section hero is-light">
  <div class="container is-max-desktop content">
    <h2 class="title">Experiments</h2>
    We conduct extensive evaluations on various question-answering tasks in finance, biomedicine, open-generation, and multi-hop reasoning domains using open-source models Llama2-7B and Mistral-7B. Our method shows substantial improvements over baseline approaches in retrieval, fine-tuning, and continual pre-training tasks. The experimental results demonstrate that synthetic knowledge ingestion significantly refines LLMs’ factual accuracy and knowledge retention across diverse question-answering datasets.
    <br><br>
    <div style="display: flex; justify-content: center;">
      <img src="static/images/rag.png" alt="Result 1"/>
  </div>
    <br>
    <p>End-to-end RAG Results(f1 score) with Two Retrievers</p>
    <table class="custom-table"> <colgroup> <col class="method-col"> <col class="score-col"> <col class="score-col"> <col class="score-col"> </colgroup> <thead> <tr> <th>Model Method</th> <th>BioASQ</th> <th>NQ</th> <th>HotpotQA</th> </tr> </thead> <tbody> <tr> <td><strong>Contriever + GPT-3.5</strong></td> <td></td> <td></td> <td></td> </tr> <tr> <td>Raw Article</td> <td>0.361</td> <td>0.504</td> <td>0.585</td> </tr> <tr> <td>Ski-Q-1 (iHyDE)</td> <td>0.317</td> <td>0.452</td> <td>0.507</td> </tr> <tr> <td>Ski-QC-1</td> <td>0.300</td> <td>0.553</td> <td>0.553</td> </tr> <tr> <td><strong>Ski-QC-ASM</strong></td> <td><strong>0.385</strong></td> <td><strong>0.563</strong></td> <td><strong>0.627</strong></td> </tr> <tr> <td><strong>BM25 + GPT-3.5</strong></td> <td></td> <td></td> <td></td> </tr> <tr> <td>Raw Article</td> <td>0.227</td> <td>0.258</td> <td>0.443</td> </tr> <tr> <td>Ski-Q-1 (iHyDE)</td> <td>0.162</td> <td>0.181</td> <td>0.274</td> </tr> <tr> <td>Ski-QC-1</td> <td>0.186</td> <td>0.201</td> <td>0.377</td> </tr> <tr> <td><strong>Ski-QC-ASM</strong></td> <td><strong>0.243</strong></td> <td><strong>0.306</strong></td> <td><strong>0.478</strong></td> </tr> </tbody> </table>
    <br>
    <p>SFT Performance(f1 score) on Two Pre-trained Models</p>
    <table class="custom-table"> <colgroup> <col class="method-col"> <col class="score-col"> <col class="score-col"> <col class="score-col"> </colgroup> <thead> <tr> <th>Model Method</th> <th>BioASQ</th> <th>NQ</th> <th>HotpotQA</th> </tr> </thead> <tbody> <tr> <td><strong>Mistral-7B</strong></td> <td></td> <td></td> <td></td> </tr> <tr> <td>Base Model</td> <td>0.125</td> <td>0.098</td> <td>0.113</td> </tr> <tr> <td>Vanilla QA</td> <td>0.131</td> <td>0.084</td> <td>0.155</td> </tr> <tr> <td>Ski-QA-1</td> <td><strong>0.162</strong></td> <td><strong>0.159</strong></td> <td><strong>0.171</strong></td> </tr> <tr> <td>Ski-QC-1</td> <td>0.138</td> <td>0.113</td> <td>0.155</td> </tr> <tr> <td>Ski-QCA-1</td> <td>0.160</td> <td>0.157</td> <td>0.163</td> </tr> <tr> <td><strong>Llama2-7B</strong></td> <td></td> <td></td> <td></td> </tr> <tr> <td>Base Model</td> <td>0.123</td> <td>0.082</td> <td>0.136</td> </tr> <tr> <td>Vanilla QA</td> <td>0.235</td> <td>0.150</td> <td>0.203</td> </tr> <tr> <td>Ski-QA-1</td> <td>0.357</td> <td><strong>0.218</strong></td> <td><strong>0.266</strong></td> </tr> <tr> <td><strong>Ski-QC-1</strong></td> <td><strong>0.436</strong></td> <td>0.217</td> <td>0.258</td> </tr> <tr> <td>Ski-QCA-1</td> <td>0.271</td> <td>0.201</td> <td>0.252</td> </tr> </tbody> </table>
    <br>
    <p>CPT Performance(f1 score) on Llama2-7B</p>
    <table class="custom-table"> <colgroup> <col class="method-col"> <col class="score-col"> <col class="score-col"> <col class="score-col"> </colgroup> <thead> <tr> <th>Method</th> <th>BioASQ</th> <th>NQ</th> <th>HotpotQA</th> </tr> </thead> <tbody> <tr> <td>Base Model</td> <td>0.123</td> <td>0.082</td> <td>0.136</td> </tr> <tr> <td>Raw Article (only context)</td> <td>0.219</td> <td>0.166</td> <td>0.242</td> </tr> <tr> <td>Ski-C-1 (1-gram context)</td> <td>0.208</td> <td>0.178</td> <td>0.204</td> </tr> <tr> <td><strong>Ski-C-ASM (1-3-gram context)</strong></td> <td>0.241</td> <td>0.163</td> <td><strong>0.253</strong></td> </tr> <tr> <td><strong>Ski-QA-1</strong></td> <td>0.269</td> <td>0.184</td> <td>0.205</td> </tr> <tr> <td>Ski-QA-ASM</td> <td>0.228</td> <td>0.194</td> <td>0.218</td> </tr> <tr> <td><strong>Ski-QC-1</strong></td> <td>0.294</td> <td>0.178</td> <td>0.226</td> </tr> <tr> <td><strong>Ski-QC-ASM</strong></td> <td><strong>0.335</strong></td> <td>0.182</td> <td>0.215</td> </tr> <tr> <td>Ski-QCA-1</td> <td>0.211</td> <td>0.188</td> <td>0.218</td> </tr> <tr> <td>Ski-QCA-ASM</td> <td>0.220</td> <td><strong>0.197</strong></td> <td>0.191</td> </tr> </tbody> </table>
  </div>
</section>


<!-- Paper poster -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>

      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{zhang2024syntheticknowledgeingestionknowledge,
        title={Synthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models},
        author={Jiaxin Zhang and Wendi Cui and Yiran Huang and Kamalika Das and Sricharan Kumar},
        year={2024},
        eprint={2410.09629},
        archivePrefix={arXiv},
        primaryClass={cs.CL},
        url={https://arxiv.org/abs/2410.09629},
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
